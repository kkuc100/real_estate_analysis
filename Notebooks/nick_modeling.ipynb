{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          DOM  SQFT  BEDS  BATHS  AGE  zip_cluster\n",
      "0  242.509910  1368     2    2.0    3       2474.0\n",
      "1  239.892049  1850     2    2.5    5       2124.0\n",
      "2  236.463293  1469     2    1.0   38       2184.0\n",
      "3  238.368257  2945     2    2.5    4       1949.0\n",
      "4   56.000000  2536     3    2.5    3       2139.0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10799 entries, 0 to 10798\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   DOM          10697 non-null  float64\n",
      " 1   SQFT         10799 non-null  int64  \n",
      " 2   BEDS         10799 non-null  int64  \n",
      " 3   BATHS        10799 non-null  float64\n",
      " 4   AGE          10799 non-null  int64  \n",
      " 5   zip_cluster  10799 non-null  float64\n",
      "dtypes: float64(3), int64(3)\n",
      "memory usage: 506.3 KB\n",
      "None\n",
      "                DOM          SQFT          BEDS         BATHS           AGE  \\\n",
      "count  10697.000000  10799.000000  10799.000000  10799.000000  10799.000000   \n",
      "mean      49.771103   1347.342439      2.086489      1.734651     48.019076   \n",
      "std       52.562444   1474.993050      0.734866      0.733142    250.632482   \n",
      "min        1.000000      0.000000      0.000000      0.500000  -7981.000000   \n",
      "25%       17.000000    903.000000      2.000000      1.000000     15.000000   \n",
      "50%       30.000000   1193.000000      2.000000      1.500000     35.000000   \n",
      "75%       62.000000   1634.000000      2.000000      2.000000     98.000000   \n",
      "max      243.037191  99999.000000      6.000000     20.000000   1828.000000   \n",
      "\n",
      "        zip_cluster  \n",
      "count  10799.000000  \n",
      "mean    2067.158996  \n",
      "std      282.387756  \n",
      "min     1420.000000  \n",
      "25%     1867.000000  \n",
      "50%     2118.000000  \n",
      "75%     2155.000000  \n",
      "max     2780.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../Data/baseline_model/CC_baseline_header_long_tail.csv')\n",
    "\n",
    "# Explore the data\n",
    "print(df.head())       # Check the first few rows\n",
    "print(df.info())       # Check data types and non-null counts\n",
    "print(df.describe())   # Summary statistics for numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOM            102\n",
      "SQFT             0\n",
      "BEDS             0\n",
      "BATHS            0\n",
      "AGE              0\n",
      "zip_cluster      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())   # Count of NaN values per column\n",
    "\n",
    "# Replace infinite values and drop rows with NaNs in critical columns (e.g., 'DOM')\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(subset=['DOM'])  # Remove rows with missing 'DOM'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AGE  AGE_normalized  SQFT  SQFT_normalized\n",
      "0    3        0.813946  1368          0.01368\n",
      "1    5        0.814150  1850          0.01850\n",
      "2   38        0.817515  1469          0.01469\n",
      "3    4        0.814048  2945          0.02945\n",
      "4    3        0.813946  2536          0.02536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply normalization to AGE and SQFT\n",
    "df[['AGE_normalized', 'SQFT_normalized']] = scaler.fit_transform(df[['AGE', 'SQFT']])\n",
    "\n",
    "# Check normalized values\n",
    "print(df[['AGE', 'AGE_normalized', 'SQFT', 'SQFT_normalized']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SQFT_BEDS_interaction  AGE_BATHS_interaction\n",
      "0               0.027360               1.627893\n",
      "1               0.037000               2.035376\n",
      "2               0.029380               0.817515\n",
      "3               0.058901               2.035121\n",
      "4               0.076081               2.034866\n"
     ]
    }
   ],
   "source": [
    "# Example: Interaction between normalized square footage and bedrooms\n",
    "df['SQFT_BEDS_interaction'] = df['SQFT_normalized'] * df['BEDS']\n",
    "\n",
    "# Example: Interaction between normalized age and bathrooms\n",
    "df['AGE_BATHS_interaction'] = df['AGE_normalized'] * df['BATHS']\n",
    "\n",
    "# Check new interaction features\n",
    "print(df[['SQFT_BEDS_interaction', 'AGE_BATHS_interaction']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to 'DOM' to reduce the impact of outliers\n",
    "df['DOM_log'] = np.log1p(df['DOM'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AGE_normalized AGE_binned\n",
      "0        0.813946        Old\n",
      "1        0.814150        Old\n",
      "2        0.817515        Old\n",
      "3        0.814048        Old\n",
      "4        0.813946        Old\n"
     ]
    }
   ],
   "source": [
    "# Bin 'AGE_normalized' into categories\n",
    "age_bins = [0, 0.3, 0.7, 1]\n",
    "age_labels = ['New', 'Mid-Age', 'Old']\n",
    "df['AGE_binned'] = pd.cut(df['AGE_normalized'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Check the binned column\n",
    "print(df[['AGE_normalized', 'AGE_binned']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   zip_cluster  zip_cluster_target_encoded\n",
      "0       2474.0                   48.130346\n",
      "1       2124.0                   39.604167\n",
      "2       2184.0                   54.957674\n",
      "3       1949.0                   58.492998\n",
      "4       2139.0                   45.858559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define target encoding function\n",
    "def target_encode(df, cat_column, target_column, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    encoded_col = np.zeros(df.shape[0])\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(df):\n",
    "        train_data, valid_data = df.iloc[train_idx], df.iloc[valid_idx]\n",
    "        mean_target = train_data.groupby(cat_column)[target_column].mean()\n",
    "        encoded_col[valid_idx] = valid_data[cat_column].map(mean_target)\n",
    "    \n",
    "    return encoded_col\n",
    "\n",
    "# Apply target encoding to 'zip_cluster'\n",
    "df['zip_cluster_target_encoded'] = target_encode(df, 'zip_cluster', 'DOM')\n",
    "# Check the encoded column\n",
    "print(df[['zip_cluster', 'zip_cluster_target_encoded']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SQFT_normalized  SQFT_normalized  AGE_normalized  AGE_normalized  \\\n",
      "0          0.01368          0.01368        0.813946        0.813946   \n",
      "1          0.01850          0.01850        0.814150        0.814150   \n",
      "2          0.01469          0.01469        0.817515        0.817515   \n",
      "3          0.02945          0.02945        0.814048        0.814048   \n",
      "4          0.02536          0.02536        0.813946        0.813946   \n",
      "\n",
      "   SQFT_normalized^2  SQFT_normalized AGE_normalized  AGE_normalized^2  \n",
      "0           0.000187                        0.011135          0.662509  \n",
      "1           0.000342                        0.015062          0.662841  \n",
      "2           0.000216                        0.012009          0.668330  \n",
      "3           0.000867                        0.023974          0.662675  \n",
      "4           0.000643                        0.020642          0.662509  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Define features for polynomial transformation\n",
    "poly_features = ['SQFT_normalized', 'AGE_normalized']\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Transform the data and add it back to the dataframe\n",
    "poly_transformed = poly.fit_transform(df[poly_features])\n",
    "poly_columns = poly.get_feature_names_out(poly_features)\n",
    "\n",
    "df_poly = pd.DataFrame(poly_transformed, columns=poly_columns)\n",
    "df = pd.concat([df, df_poly], axis=1)\n",
    "\n",
    "# Check polynomial features\n",
    "print(df[poly_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DOM', 'SQFT', 'BEDS', 'BATHS', 'AGE', 'zip_cluster', 'AGE_normalized',\n",
      "       'SQFT_normalized', 'SQFT_BEDS_interaction', 'AGE_BATHS_interaction',\n",
      "       'DOM_log', 'AGE_binned', 'zip_cluster_target_encoded',\n",
      "       'SQFT_normalized', 'AGE_normalized', 'SQFT_normalized^2',\n",
      "       'SQFT_normalized AGE_normalized', 'AGE_normalized^2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8639, 13)\n",
      "X_test shape: (2160, 13)\n",
      "y_train shape: (8639,)\n",
      "y_test shape: (2160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and target variable\n",
    "features = [\n",
    "    'SQFT_normalized', 'BEDS', 'BATHS', 'AGE_normalized', 'zip_cluster_target_encoded',\n",
    "    'SQFT_BEDS_interaction', 'AGE_BATHS_interaction', 'AGE_binned',\n",
    "    'SQFT_normalized^2', 'SQFT_normalized AGE_normalized', 'AGE_normalized^2'\n",
    "]\n",
    "target = 'DOM'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the shapes of the resulting splits for verification\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 2, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/186624 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Expanded parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],                # Number of trees\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],           # Step size shrinkage\n",
    "    'max_depth': [3, 5, 7, 10],                        # Maximum depth of trees\n",
    "    'subsample': [0.6, 0.8, 1],                        # Fraction of samples used per tree\n",
    "    'colsample_bytree': [0.6, 0.8, 1],                 # Fraction of features used per tree\n",
    "    'gamma': [0, 0.1, 0.3, 0.5],                       # Minimum loss reduction for split\n",
    "    'min_child_weight': [1, 3, 5],                     # Minimum sum of weights in a child\n",
    "    'reg_alpha': [0, 0.01, 0.1],                       # L1 regularization term\n",
    "    'reg_lambda': [1, 1.5, 2],                         # L2 regularization term\n",
    "    'scale_pos_weight': [1],                           # Class imbalance scaling\n",
    "    'tree_method': ['exact', 'approx', 'hist'],        # Tree construction algorithm\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_regressor,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=0,  # Disable default verbose output\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Wrap the fitting process with a tqdm progress bar\n",
    "with tqdm(total=len(param_grid['n_estimators']) * len(param_grid['learning_rate']) * len(param_grid['max_depth']) * \n",
    "          len(param_grid['subsample']) * len(param_grid['colsample_bytree']) * len(param_grid['gamma']) * \n",
    "          len(param_grid['min_child_weight']) * len(param_grid['reg_alpha']) * len(param_grid['reg_lambda']) * \n",
    "          len(param_grid['tree_method'])) as pbar:\n",
    "    def tqdm_callback(*args, **kwargs):\n",
    "        pbar.update(1)\n",
    "\n",
    "    grid_search.fit(\n",
    "        train.drop(columns=['DOM']),\n",
    "        train['DOM']\n",
    "    )\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best RMSE Score:\", -grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
